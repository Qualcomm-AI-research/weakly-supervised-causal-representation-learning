{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/antares_raid/home/jonasb/mambaforge/envs/wscrl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/antares_raid/home/jonasb/Projects/wscrl/repo/experiments/cubes_2d.py:50: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(\n"
     ]
    }
   ],
   "source": [
    "from experiments.cubes_2d import get_dataloader, create_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ws_crl.posthoc_graph_learning.enco import run_enco\n",
    "import logging\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# EXP_DIR = Path(\"/mnt/raid/ni/jonasb/wscrl/saved/sbd_complete\")\n",
    "EXP_DIR = Path(\n",
    "    \"/mnt/raid/ni/jonasb/wscrl/son_causal_variables_2dcubes_allow_collisions_10k_0.5_child_noise\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ILCM(\n",
       "  (scm): MLPImplicitSCM(\n",
       "    (solution_functions): ModuleList(\n",
       "      (0-5): 6 x ConditionalAffineScalarTransform(\n",
       "        (param_net): Sequential(\n",
       "          (0): Linear(in_features=12, out_features=100, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=100, out_features=100, bias=True)\n",
       "          (3): ReLU()\n",
       "          (4): Linear(in_features=100, out_features=2, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (base_density): StandardNormal()\n",
       "  )\n",
       "  (intervention_prior): InterventionPrior()\n",
       "  (encoder): GaussianEncoder(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=6, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (9): ReLU()\n",
       "    )\n",
       "    (mean_head): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (std_head): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=6, bias=True)\n",
       "      (1): Softplus(beta=1, threshold=20)\n",
       "    )\n",
       "  )\n",
       "  (decoder): GaussianEncoder(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=6, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (9): ReLU()\n",
       "    )\n",
       "    (mean_head): Linear(in_features=64, out_features=6, bias=True)\n",
       "    (std_head): Linear(in_features=64, out_features=6, bias=True)\n",
       "  )\n",
       "  (intervention_encoder): HeuristicInterventionEncoder()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = EXP_DIR / \"models\" / \"model.pt\"\n",
    "cfg_path = EXP_DIR / \"config.yml\"\n",
    "\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "\n",
    "model = create_model(cfg)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_enco_graph(cfg, model, partition=\"train\"):\n",
    "    \"\"\"Post-hoc graph evaluation with ENCO\"\"\"\n",
    "\n",
    "    # Only want to do this for ILCMs\n",
    "    if cfg.model.type not in [\"intervention_noise_vae\", \"alt_intervention_noise_vae\"]:\n",
    "        return {}\n",
    "\n",
    "    # Let's skip this for large latent spaces\n",
    "    if cfg.model.dim_z > 8:\n",
    "        return {}\n",
    "\n",
    "    logger.info(\"Evaluating learned graph with ENCO\")\n",
    "\n",
    "    model.eval()\n",
    "    device = torch.device(cfg.training.device)\n",
    "    cpu = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Load data and compute causal variables\n",
    "    dataloader = get_dataloader(cfg, partition, cfg.eval.batchsize)\n",
    "    z0s, z1s, interventions = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x0, x1, *_ in dataloader:\n",
    "            x0, x1 = x0.to(device), x1.to(device)\n",
    "            _, _, _, _, e0, e1, _, _, intervention = model.encode_decode_pair(\n",
    "                x0.to(device), x1.to(device)\n",
    "            )\n",
    "            z0 = model.scm.noise_to_causal(e0)\n",
    "            z1 = model.scm.noise_to_causal(e1)\n",
    "\n",
    "            z0s.append(z0.to(cpu))\n",
    "            z1s.append(z1.to(cpu))\n",
    "            interventions.append(intervention.to(cpu))\n",
    "\n",
    "        z0s = torch.cat(z0s, dim=0).detach()\n",
    "        z1s = torch.cat(z1s, dim=0).detach()\n",
    "        interventions = torch.cat(interventions, dim=0).detach()\n",
    "\n",
    "    # Run ENCO\n",
    "    adjacency_matrix = (\n",
    "        run_enco(z0s, z1s, interventions, lambda_sparse=cfg.eval.enco_lambda, device=device)\n",
    "        .cpu()\n",
    "        .detach()\n",
    "    )\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "    # Package as dict\n",
    "    # results = {\n",
    "    #     f\"enco_graph_{i}_{j}\": adjacency_matrix[i, j].item()\n",
    "    #     for i in range(model.dim_z)\n",
    "    #     for j in range(model.dim_z)\n",
    "    # }\n",
    "    # return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_enco_graph(cfg, model, partition=cfg.eval.eval_partition)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wscrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
